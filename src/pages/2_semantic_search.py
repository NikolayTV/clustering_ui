import re, os
import streamlit as st
import pandas as pd
import glob
import json 

from data_utils import load_h5_to_dataframe, representative_tfidf_ngrams, cosine_similarity_1d, load_to_df, semantic_search
from llm_utils import get_embedding_local, get_embedding_runpod

with open('src/config.json', 'r') as f:
    config = json.load(f)

# File Upload Section
st.set_page_config(page_title="–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö", page_icon="üè†")
st.title("–¢–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ —á–∞—Ç–æ–≤")

# ROWS_LIMIT = st.sidebar.text_input("–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è", value=100)
# try:
#     ROWS_LIMIT = int(ROWS_LIMIT)
# except ValueError:
#     st.error("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤–≤–µ–¥–∏—Ç–µ —á–∏—Å–ª–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å—Ç—Ä–æ–∫")
#     ROWS_LIMIT = 100
ROWS_LIMIT = 100

@st.cache_data
def load_data(uploaded_file):
    data_load_state = st.text('Loading data...')

    data = pd.read_csv(uploaded_file)
    if 'question' not in data.columns.tolist():
        st.error('data should contain column: sender')
    elif 'answer' not in data.columns.tolist():
        st.error('data should contain column: body')
    data_load_state.text('Loading data...done!')
    data = data.dropna(subset=['sender', 'body'])
    return data

@st.cache_data
def apply_fts(filtered_data, white_list, black_list):

    if len(white_list):
        white_list_words = white_list.split(';')
        fts_pattern = '|'.join(map(re.escape, white_list_words))
        filtered_data = filtered_data[filtered_data['questions'].str.contains(fts_pattern, case=False, regex=True)]
    print('1', filtered_data.shape)

    if len(black_list):
        black_list_words = black_list.split(';')
        blk_pattern = '|'.join(map(re.escape, black_list_words))
        filtered_data = filtered_data[~filtered_data['questions'].str.contains(blk_pattern, case=False, regex=True)]

    print('3', filtered_data.shape)
    return filtered_data

@st.experimental_fragment
def show_data_toggle(data, rows_limit):
    show_original = st.toggle(label='–ü–æ–∫–∞–∑–∞—Ç—å –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ', key='toggle_data')
    if show_original:
        st.dataframe(data[:rows_limit])


if 'df' not in st.session_state: st.session_state['df'] = None
if 'filtered_data' not in st.session_state: st.session_state['filtered_data'] = None
if '–ü—Ä–∏–º–µ–Ω–∏—Ç—å —Ñ–∏–ª—å—Ç—Ä—ã' not in st.session_state: st.session_state['–ü—Ä–∏–º–µ–Ω–∏—Ç—å —Ñ–∏–ª—å—Ç—Ä—ã'] = None

# < DATA LOADING >
st.markdown('### –í—ã–±–µ—Ä–∏—Ç–µ —Ñ–∞–π–ª—ã –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏')
files = glob.glob('saved_data/**/*')

with st.form("data_form"):
    selected_files = st.multiselect('Select files to load', files)
    upload_button = st.form_submit_button('–ó–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ')
    if upload_button:
        if selected_files:
            df = load_to_df(selected_files)
            st.session_state['df'] = df
if 'df' in st.session_state and st.session_state['df'] is not None:
    df = st.session_state['df']
    st.write('–ó–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ:', df.shape, df.sample(1))
# < / DATA LOADING >

# LOGGING

# show_original = st.checkbox('–ü–æ–∫–∞–∑–∞—Ç—å –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ')
# if show_original:
#     st.dataframe(data[:ROWS_LIMIT])

if 'df' in st.session_state and st.session_state['df'] is not None:
    filtered_data = st.session_state['df'].copy()
    show_data_toggle(filtered_data, ROWS_LIMIT)

    # FILTERS
    with st.form("filters_form"):

        st.header("2. –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è")
        st.subheader("–ü–æ–ª–Ω–æ-—Ç–µ–∫—Å—Ç–æ–≤–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è")
        white_list = st.text_input("–ë–µ–ª—ã–π —Å–ø–∏—Å–æ–∫ (–∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å ;)")
        black_list = st.text_input("–ß–µ—Ä–Ω—ã–π —Å–ø–∏—Å–æ–∫ (–∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å ;)")

        st.subheader("–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è")
        semantic_query = st.text_input("–û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞—Ç—å –ø–æ —Å–º—ã—Å–ª—É")
        semantic_threshold = st.slider("–ü–æ—Ä–æ–≥ –ø–æ—Ö–æ–∂–µ—Å—Ç–∏", 0.0, 1.0, 0.7)
        apply_filters = st.form_submit_button('–ü—Ä–∏–º–µ–Ω–∏—Ç—å —Ñ–∏–ª—å—Ç—Ä—ã')

    if apply_filters:
        if isinstance(semantic_query, str) and len(semantic_query) > 1:
            print('semantic_query', semantic_query)
            # < SEMANTIC SEARCH >
            if config.get('use_local_emb_model'):
                res = get_embedding_local(semantic_query)
            else:
                res = get_embedding_runpod(semantic_query)
            target_embedding = res['embeddings']

            filtered_data = semantic_search(filtered_data, target_embedding)
            filtered_data = filtered_data[filtered_data['cos_sim'] > semantic_threshold]
            st.write(f"Semantic filtering shape:", filtered_data.shape)
            # < / SEMANTIC SEARCH >

        if len(white_list) or len(black_list):
            # < FTS >
            filtered_data = apply_fts(filtered_data, white_list, black_list)
            st.write(f"Full text search shape:", filtered_data.shape)
            # < / FTS >

        st.dataframe(filtered_data[:ROWS_LIMIT])
        st.session_state['filtered_data'] = filtered_data


    # SAVE
    st.sidebar.header("3. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ")
    save_path = st.sidebar.text_input("–ü—É—Ç—å –∏ –Ω–∞–∑–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è", value="saved_data/test/sample.csv")

    if st.sidebar.button("–°–æ—Ö—Ä–∞–Ω–∏—Ç—å"):
        if st.session_state['filtered_data'] is None:
            st.sidebar.error(f"–û—à–∏–±–∫–∞. –ü—Ä–µ–∂–¥–µ —á–µ–º —Å–æ—Ö—Ä–∞–Ω—è—Ç—å - –ø—Ä–∏–º–µ–Ω–∏—Ç–µ —Ñ–∏–ª—å—Ç—Ä—ã.")

        if st.session_state['filtered_data'] is not None:
            filtered_data = st.session_state['filtered_data']
            if filtered_data.shape[0] == 0:
                st.sidebar.error(f"–û—à–∏–±–∫–∞. –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç 0 —Å—Ç—Ä–æ—á–µ–∫")

            else:
                directory = "/".join(save_path.split('/')[:-1])
                filename = save_path.split('/')[-1]
                if not os.path.exists(directory):
                    os.makedirs(directory)

                filepath = os.path.join(directory, filename)
                filtered_data.to_csv(filepath, index=False)
                st.sidebar.success(f"–î–∞–Ω–Ω–≤—ã–µ —Å –≤—ã–±—Ä–∞–Ω–Ω—ã–º–∏ —Ñ–∏–ª—å—Ç—Ä–∞–º–∏ —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {filepath}")


